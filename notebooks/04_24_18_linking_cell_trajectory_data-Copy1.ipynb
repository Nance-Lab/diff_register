{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import diff_classifier.aws as aws\n",
    "import diff_classifier.features as ft\n",
    "import diff_classifier.msd as msd\n",
    "import diff_classifier.heatmaps as hm\n",
    "import diff_classifier.imagej as ij\n",
    "import boto3\n",
    "import os.path as op\n",
    "import cloudknot as ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track = {}\n",
    "knot = {}\n",
    "result_futures = {}\n",
    "start_knot = 918 #Must be unique number for every run on Cloudknot.\n",
    "\n",
    "slices = ['1', '2']\n",
    "regions = [1, 2, 3]\n",
    "videos = [1, 2, 3, 4, 5]\n",
    "folder = 'Tissue_Studies/04_23_18_Registration_Test/tracking' #Folder in AWS S3 containing files to be analyzed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_split_track_msds(prefix):\n",
    "    \"\"\"\n",
    "    1. Checks to see if features file exists.\n",
    "    2. If not, checks to see if image partitioning has occured.\n",
    "    3. If yes, checks to see if tracking has occured.\n",
    "    4. Regardless, tracks, calculates MSDs and features.\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib as mpl\n",
    "    #mpl.use('Agg')\n",
    "    import diff_classifier.aws as aws\n",
    "    import diff_classifier.utils as ut\n",
    "    import diff_classifier.msd as msd\n",
    "    import diff_classifier.features as ft\n",
    "    import diff_classifier.imagej as ij\n",
    "    import diff_classifier.heatmaps as hm\n",
    "\n",
    "    from scipy.spatial import Voronoi\n",
    "    import scipy.stats as stats\n",
    "    from shapely.geometry import Point\n",
    "    from shapely.geometry.polygon import Polygon\n",
    "    import matplotlib.cm as cm\n",
    "    import os\n",
    "    import os.path as op\n",
    "    import glob\n",
    "    import numpy as np\n",
    "    import numpy.ma as ma\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "\n",
    "    #Splitting section\n",
    "    ###############################################################################################\n",
    "    remote_folder = \"Tissue_Studies/04_23_18_Registration_Test/tracking\"\n",
    "    local_folder = os.getcwd()\n",
    "    ires = 512\n",
    "    frames = 651\n",
    "    filename = '{}.tif'.format(prefix)\n",
    "    remote_name = remote_folder+'/'+filename\n",
    "    local_name = local_folder+'/'+filename\n",
    "\n",
    "    msd_file = 'msd_{}.csv'.format(prefix)\n",
    "    ft_file = 'features_{}.csv'.format(prefix)\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    names = []\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "    try:\n",
    "        obj = s3.head_object(Bucket='ccurtis.data', Key=remote_folder+'/'+ft_file)\n",
    "    except:\n",
    "\n",
    "        try:\n",
    "            for name in names:\n",
    "                aws.download_s3(remote_folder+'/'+name, name, bucket_name='ccurtis.data')\n",
    "        except:\n",
    "            aws.download_s3(remote_name, local_name, bucket_name='ccurtis.data')\n",
    "            names = ij.partition_im(local_name)\n",
    "            \n",
    "            names = []\n",
    "            for i in range(0, 4):\n",
    "                for j in range(0, 4):\n",
    "                    names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "            for name in names:\n",
    "                aws.upload_s3(name, remote_folder+'/'+name, bucket_name='ccurtis.data')\n",
    "                print(\"Done with splitting.  Should output file of name {}\".format(remote_folder+'/'+name))\n",
    "\n",
    "        #Tracking section\n",
    "        ################################################################################################\n",
    "        names = []\n",
    "        for i in range(0, 4):\n",
    "                for j in range(0, 4):\n",
    "                    names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "        for name in names:\n",
    "            outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "            local_im = op.join(local_folder, name)\n",
    "\n",
    "            row = int(name.split('.')[0].split('_')[4])\n",
    "            col = int(name.split('.')[0].split('_')[5])\n",
    "\n",
    "            try:\n",
    "                aws.download_s3(remote_folder+'/'+outfile, outfile, bucket_name='ccurtis.data')\n",
    "            except:\n",
    "                test_intensity = ij.mean_intensity(local_im)\n",
    "                if test_intensity > 500:\n",
    "                    quality = 245\n",
    "                else:\n",
    "                    quality = 0.1\n",
    "\n",
    "                if row==3:\n",
    "                    y = 485\n",
    "                else:\n",
    "                    y = 511\n",
    "\n",
    "                ij.track(local_im, outfile, template=None, fiji_bin=None, radius=5.0, threshold=0.01,\n",
    "                         do_median_filtering=True, quality=quality, x=511, y=y, ylo=1, median_intensity=300.0, snr=0.0,\n",
    "                         linking_max_distance=8.0, gap_closing_max_distance=15.0, max_frame_gap=9,\n",
    "                         track_displacement=0.0)\n",
    "\n",
    "                aws.upload_s3(outfile, remote_folder+'/'+outfile, bucket_name='ccurtis.data')\n",
    "            print(\"Done with tracking.  Should output file of name {}\".format(remote_folder+'/'+outfile))\n",
    "\n",
    "\n",
    "        #MSD and features section\n",
    "        #################################################################################################\n",
    "        files_to_big = False\n",
    "        size_limit = 10\n",
    "\n",
    "        for name in names:\n",
    "            outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "            local_im = name\n",
    "            file_size_MB = op.getsize(local_im)/1000000\n",
    "            if file_size_MB > size_limit:\n",
    "                file_to_big = True\n",
    "\n",
    "        if files_to_big:\n",
    "            print('One or more of the {} trajectory files exceeds {}MB in size.  Will not continue with MSD calculations.'.format(\n",
    "                  prefix, size_limit))\n",
    "        else:\n",
    "            counter = 0\n",
    "            for name in names:\n",
    "                row = int(name.split('.')[0].split('_')[4])\n",
    "                col = int(name.split('.')[0].split('_')[5])\n",
    "\n",
    "                filename = \"Traj_{}_{}_{}.csv\".format(prefix, row, col)\n",
    "                local_name = local_folder+'/'+filename\n",
    "\n",
    "                if counter == 0:\n",
    "                    to_add = ut.csv_to_pd(local_name)\n",
    "                    to_add['X'] = to_add['X'] + ires*col\n",
    "                    to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                    merged = msd.all_msds2(to_add, frames=frames)\n",
    "                else:\n",
    "\n",
    "                    if merged.shape[0] > 0:\n",
    "                        to_add = ut.csv_to_pd(local_name)\n",
    "                        to_add['X'] = to_add['X'] + ires*col\n",
    "                        to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                        to_add['Track_ID'] = to_add['Track_ID'] + max(merged['Track_ID']) + 1\n",
    "                    else:\n",
    "                        to_add = ut.csv_to_pd(local_name)\n",
    "                        to_add['X'] = to_add['X'] + ires*col\n",
    "                        to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                        to_add['Track_ID'] = to_add['Track_ID']\n",
    "\n",
    "                    merged = merged.append(msd.all_msds2(to_add, frames=frames))\n",
    "                    print('Done calculating MSDs for row {} and col {}'.format(row, col))\n",
    "                counter = counter + 1\n",
    "\n",
    "            merged.to_csv(msd_file)\n",
    "            aws.upload_s3(msd_file, remote_folder+'/'+msd_file, bucket_name='ccurtis.data')\n",
    "            merged_ft = ft.calculate_features(merged)\n",
    "            merged_ft.to_csv(ft_file)\n",
    "\n",
    "            aws.upload_s3(ft_file, remote_folder+'/'+ft_file, bucket_name='ccurtis.data')\n",
    "\n",
    "            #Plots\n",
    "            features = ('AR', 'D_fit', 'alpha', 'MSD_ratio', 'Track_ID', 'X', 'Y', 'asymmetry1', 'asymmetry2', 'asymmetry3',\n",
    "                        'boundedness', 'efficiency', 'elongation', 'fractal_dim', 'frames', 'kurtosis', 'straightness', 'trappedness')\n",
    "\n",
    "            die = {'features': features,\n",
    "                   'vmin': vmin,\n",
    "                   'vmax': vmax}\n",
    "            di = pd.DataFrame(data=die)\n",
    "\n",
    "            hm.plot_trajectories(prefix, remote_folder=remote_folder, bucket='ccurtis.data')\n",
    "            try:\n",
    "                hm.plot_histogram(prefix, remote_folder=remote_folder, bucket='ccurtis.data')\n",
    "            except ValueError:\n",
    "                print(\"Couldn't plot histogram.\")\n",
    "            hm.plot_particles_in_frame(prefix, remote_folder=remote_folder, bucket='ccurtis.data')\n",
    "            gmean1, gSEM1 = hm.plot_individual_msds(prefix, alpha=0.05, remote_folder=remote_folder, bucket='ccurtis.data')\n",
    "            \n",
    "    for filename in glob.glob('./100*'):\n",
    "        os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cloudknot(prefix):\n",
    "    print('The run {} was successful'.format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_installs=('https://github.com/ccurtis7/diff_classifier.git')\n",
    "my_image = ck.DockerImage(func=download_split_track_msds, base_image='arokem/python3-fiji:0.3', github_installs=github_installs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_file = open(my_image.docker_path)\n",
    "docker_string = docker_file.read()\n",
    "docker_file.close()\n",
    "\n",
    "req = open(op.join(op.split(my_image.docker_path)[0], 'requirements.txt'))\n",
    "req_string = req.read()\n",
    "req.close()\n",
    "\n",
    "new_req = req_string[0:req_string.find('\\n')-4]+'5.28'+ req_string[req_string.find('\\n'):]\n",
    "req_overwrite = open(op.join(op.split(my_image.docker_path)[0], 'requirements.txt'), 'w')\n",
    "req_overwrite.write(new_req)\n",
    "req_overwrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-064929e1243a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Test Docker Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0.1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test_image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'my_image' is not defined"
     ]
    }
   ],
   "source": [
    "#Test Docker Image\n",
    "my_image.build(\"0.1\", image_name=\"test_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck.clobber(name='diff_classifier_904')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes to be loaded: 30\n",
      "Next knot name: 919\n"
     ]
    }
   ],
   "source": [
    "to_track = []\n",
    "for slic in slices:\n",
    "    for region in regions:\n",
    "        for video in videos:\n",
    "            prefix = '100nm_S{}_XY{}_{}'.format(slic, region, video)\n",
    "            to_track.append(prefix)\n",
    "\n",
    "test_length = len(to_track)\n",
    "print('Number of nodes to be loaded: {}'.format(test_length))\n",
    "\n",
    "knot = ck.Knot(name='diff_classifier_{}'.format(start_knot),\n",
    "               docker_image = my_image,\n",
    "               memory = 32000,\n",
    "               resource_type = \"SPOT\",\n",
    "               bid_percentage = 100,\n",
    "               image_id = 'ami-0de34a0a338c1051b',\n",
    "               pars_policies=('AmazonS3FullAccess',))\n",
    "               \n",
    "result_futures = knot.map(to_track)\n",
    "start_knot = start_knot + 1\n",
    "print('Next knot name: {}'.format(start_knot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot.clobber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '100nm_S1_XY2_5'\n",
    "download_split_track_msds(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/home/ubuntu/source/diff-classifier/notebooks'\n",
    "fname = 'features_P1_S1_L_0000.csv'\n",
    "file = '{}/{}'.format(folder, fname)\n",
    "features = pd.read_csv(file, index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = ['1', '2']\n",
    "regions = [1, 2, 3]\n",
    "videos = [1, 2, 3, 4, 5]\n",
    "\n",
    "for slic in slices:\n",
    "    for region in regions:\n",
    "        for video in videos:\n",
    "            prefix = '100nm_S{}_XY{}_{}'.format(slic, region, video)\n",
    "            download_split_track_msds(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_folder = \"Tissue_Studies/04_23_18_Registration_Test/tracking\"\n",
    "local_folder = os.getcwd()\n",
    "ires = 512\n",
    "frames = 651\n",
    "filename = '{}.tif'.format(prefix)\n",
    "remote_name = remote_folder+'/'+filename\n",
    "local_name = local_folder+'/'+filename\n",
    "\n",
    "msd_file = 'msd_{}.csv'.format(prefix)\n",
    "ft_file = 'features_{}.csv'.format(prefix)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "names = []\n",
    "for i in range(0, 4):\n",
    "    for j in range(0, 4):\n",
    "        names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "try:\n",
    "    obj = s3.head_object(Bucket='ccurtis.data', Key=remote_folder+'/'+ft_file)\n",
    "except:\n",
    "\n",
    "    try:\n",
    "        for name in names:\n",
    "            aws.download_s3(remote_folder+'/'+name, name, bucket_name='ccurtis.data')\n",
    "    except:\n",
    "        aws.download_s3(remote_name, local_name, bucket_name='ccurtis.data')\n",
    "        names = ij.partition_im(local_name)\n",
    "        names = []\n",
    "        for i in range(0, 4):\n",
    "            for j in range(0, 4):\n",
    "                names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "        for name in names:\n",
    "            aws.upload_s3(name, remote_folder+'/'+name, bucket_name='ccurtis.data')\n",
    "            print(\"Done with splitting.  Should output file of name {}\".format(remote_folder+'/'+name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm 100*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm feat*\n",
    "!rm msd*\n",
    "!rm Traj*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '100nm_S1_XY2_5_1_1'\n",
    "local_folder = '.'\n",
    "name = \"{}.tif\".format(prefix)\n",
    "outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "local_im = op.join(local_folder, name)\n",
    "quality = 0\n",
    "y=511\n",
    "ij.track(local_im, outfile, template=None, fiji_bin=None, radius=5, threshold=0.1,\n",
    "                         do_median_filtering=True, quality=quality, x=511, y=y, ylo=1, median_intensity=300.0, snr=0.0,\n",
    "                         linking_max_distance=10.0, gap_closing_max_distance=12.0, max_frame_gap=9,\n",
    "                         track_displacement=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '100nm_S1_XY2_5_1_2'\n",
    "filename = '{}.tif'.format(prefix)\n",
    "fname = \"Tissue_Studies/04_23_18_Registration_Test/tracking/{}\".format(filename)\n",
    "\n",
    "aws.download_s3(fname, filename, bucket_name='ccurtis.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_folder = '.'\n",
    "name = \"{}.tif\".format(prefix)\n",
    "outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "local_im = op.join(local_folder, name)\n",
    "quality = 0.1\n",
    "y=511\n",
    "ij.track(local_im, outfile, template=None, fiji_bin=None, radius=5.0, threshold=0.01,\n",
    "                         do_median_filtering=False, quality=quality, x=511, y=y, ylo=1, median_intensity=300.0, snr=0.0,\n",
    "                         linking_max_distance=8.0, gap_closing_max_distance=15.0, max_frame_gap=9,\n",
    "                         track_displacement=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
