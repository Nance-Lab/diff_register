{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import diff_classifier.aws as aws\n",
    "import diff_classifier.features as ft\n",
    "import diff_classifier.msd as msd\n",
    "import diff_classifier.heatmaps as hm\n",
    "import diff_classifier.imagej as ij\n",
    "import boto3\n",
    "import os.path as op\n",
    "import cloudknot as ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track = {}\n",
    "knot = {}\n",
    "result_futures = {}\n",
    "start_knot = 1 #Must be unique number for every run on Cloudknot.\n",
    "\n",
    "slices = ['1', '2']\n",
    "regions = [1, 2, 3]\n",
    "videos = [1, 2, 3, 4, 5]\n",
    "folder = 'Tissue_Studies/04_23_18_Registration_Test/tracking' #Folder in AWS S3 containing files to be analyzed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_split_track_msds(prefix):\n",
    "    \"\"\"\n",
    "    1. Checks to see if features file exists.\n",
    "    2. If not, checks to see if image partitioning has occured.\n",
    "    3. If yes, checks to see if tracking has occured.\n",
    "    4. Regardless, tracks, calculates MSDs and features.\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib as mpl\n",
    "    mpl.use('Agg')\n",
    "    import diff_classifier.aws as aws\n",
    "    import diff_classifier.utils as ut\n",
    "    import diff_classifier.msd as msd\n",
    "    import diff_classifier.features as ft\n",
    "    import diff_classifier.imagej as ij\n",
    "    import diff_classifier.heatmaps as hm\n",
    "\n",
    "    from scipy.spatial import Voronoi\n",
    "    import scipy.stats as stats\n",
    "    from shapely.geometry import Point\n",
    "    from shapely.geometry.polygon import Polygon\n",
    "    import matplotlib.cm as cm\n",
    "    import os\n",
    "    import os.path as op\n",
    "    import numpy as np\n",
    "    import numpy.ma as ma\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "\n",
    "    #Splitting section\n",
    "    ###############################################################################################\n",
    "    remote_folder = \"Tissue_Studies/04_23_18_Registration_Test/tracking\"\n",
    "    local_folder = os.getcwd()\n",
    "    ires = 512\n",
    "    frames = 651\n",
    "    filename = '{}.tif'.format(prefix)\n",
    "    remote_name = remote_folder+'/'+filename\n",
    "    local_name = local_folder+'/'+filename\n",
    "\n",
    "    msd_file = 'msd_{}.csv'.format(prefix)\n",
    "    ft_file = 'features_{}.csv'.format(prefix)\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    names = []\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "    try:\n",
    "        obj = s3.head_object(Bucket='ccurtis.data', Key=remote_folder+'/'+ft_file)\n",
    "    except:\n",
    "\n",
    "        try:\n",
    "            for name in names:\n",
    "                aws.download_s3(remote_folder+'/'+name, name, bucket_name='ccurtis.data')\n",
    "        except:\n",
    "            aws.download_s3(remote_name, local_name, bucket_name='ccurtis.data')\n",
    "            names = ij.partition_im(local_name)\n",
    "            for i in range(0, 4):\n",
    "                for j in range(0, 4):\n",
    "                    names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "            for name in names:\n",
    "                aws.upload_s3(name, remote_folder+'/'+name, bucket_name='ccurtis.data')\n",
    "                print(\"Done with splitting.  Should output file of name {}\".format(remote_folder+'/'+name))\n",
    "\n",
    "        #Tracking section\n",
    "        ################################################################################################\n",
    "        \n",
    "        for i in range(0, 4):\n",
    "                for j in range(0, 4):\n",
    "                    names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "        for name in names:\n",
    "            outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "            local_im = op.join(local_folder, name)\n",
    "\n",
    "            row = int(name.split('.')[0].split('_')[4])\n",
    "            col = int(name.split('.')[0].split('_')[5])\n",
    "\n",
    "            try:\n",
    "                aws.download_s3(remote_folder+'/'+outfile, outfile, bucket_name='ccurtis.data')\n",
    "            except:\n",
    "                test_intensity = ij.mean_intensity(local_im)\n",
    "                if test_intensity > 500:\n",
    "                    quality = 245\n",
    "                else:\n",
    "                    quality = 0\n",
    "\n",
    "                if row==3:\n",
    "                    y = 485\n",
    "                else:\n",
    "                    y = 511\n",
    "\n",
    "                ij.track(local_im, outfile, template=None, fiji_bin=None, radius=5, threshold=0.1,\n",
    "                         do_median_filtering=True, quality=quality, x=511, y=y, ylo=1, median_intensity=300.0, snr=0.0,\n",
    "                         linking_max_distance=10.0, gap_closing_max_distance=12.0, max_frame_gap=9,\n",
    "                         track_displacement=0.0)\n",
    "\n",
    "                aws.upload_s3(outfile, remote_folder+'/'+outfile, bucket_name='ccurtis.data')\n",
    "            print(\"Done with tracking.  Should output file of name {}\".format(remote_folder+'/'+outfile))\n",
    "\n",
    "\n",
    "        #MSD and features section\n",
    "        #################################################################################################\n",
    "        files_to_big = False\n",
    "        size_limit = 10\n",
    "\n",
    "        for name in names:\n",
    "            outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "            local_im = name\n",
    "            file_size_MB = op.getsize(local_im)/1000000\n",
    "            if file_size_MB > size_limit:\n",
    "                file_to_big = True\n",
    "\n",
    "        if files_to_big:\n",
    "            print('One or more of the {} trajectory files exceeds {}MB in size.  Will not continue with MSD calculations.'.format(\n",
    "                  prefix, size_limit))\n",
    "        else:\n",
    "            counter = 0\n",
    "            for name in names:\n",
    "                row = int(name.split('.')[0].split('_')[4])\n",
    "                col = int(name.split('.')[0].split('_')[5])\n",
    "\n",
    "                filename = \"Traj_{}_{}_{}.csv\".format(prefix, row, col)\n",
    "                local_name = local_folder+'/'+filename\n",
    "\n",
    "                if counter == 0:\n",
    "                    to_add = ut.csv_to_pd(local_name)\n",
    "                    to_add['X'] = to_add['X'] + ires*col\n",
    "                    to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                    merged = msd.all_msds2(to_add, frames=frames)\n",
    "                else:\n",
    "\n",
    "                    if merged.shape[0] > 0:\n",
    "                        to_add = ut.csv_to_pd(local_name)\n",
    "                        to_add['X'] = to_add['X'] + ires*col\n",
    "                        to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                        to_add['Track_ID'] = to_add['Track_ID'] + max(merged['Track_ID']) + 1\n",
    "                    else:\n",
    "                        to_add = ut.csv_to_pd(local_name)\n",
    "                        to_add['X'] = to_add['X'] + ires*col\n",
    "                        to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                        to_add['Track_ID'] = to_add['Track_ID']\n",
    "\n",
    "                    merged = merged.append(msd.all_msds2(to_add, frames=frames))\n",
    "                    print('Done calculating MSDs for row {} and col {}'.format(row, col))\n",
    "                counter = counter + 1\n",
    "\n",
    "            merged.to_csv(msd_file)\n",
    "            aws.upload_s3(msd_file, remote_folder+'/'+msd_file, bucket_name='ccurtis.data')\n",
    "            merged_ft = ft.calculate_features(merged)\n",
    "            merged_ft.to_csv(ft_file)\n",
    "\n",
    "            aws.upload_s3(ft_file, remote_folder+'/'+ft_file, bucket_name='ccurtis.data')\n",
    "\n",
    "            #Plots\n",
    "            features = ('AR', 'D_fit', 'alpha', 'MSD_ratio', 'Track_ID', 'X', 'Y', 'asymmetry1', 'asymmetry2', 'asymmetry3',\n",
    "                        'boundedness', 'efficiency', 'elongation', 'fractal_dim', 'frames', 'kurtosis', 'straightness', 'trappedness')\n",
    "            vmin = (1.36, 0.015, 0.72, -0.09, 0, 0, 0, 0.5, 0.049, 0.089, 0.0069, 0.65, 0.26, 1.28, 0, 1.66, 0.087, -0.225)\n",
    "            vmax = (3.98, 2.6, 2.3, 0.015, max(merged_ft['Track_ID']), 2048, 2048, 0.99, 0.415, 0.53,\n",
    "                    0.062, 3.44, 0.75, 1.79, 650, 3.33, 0.52, -0.208)\n",
    "            die = {'features': features,\n",
    "                   'vmin': vmin,\n",
    "                   'vmax': vmax}\n",
    "            di = pd.DataFrame(data=die)\n",
    "            for i in range(0, di.shape[0]):\n",
    "                hm.plot_heatmap(prefix, feature=di['features'][i], vmin=di['vmin'][i], vmax=di['vmax'][i],\n",
    "                                remote_folder=remote_folder, bucket='ccurtis.data')\n",
    "                hm.plot_scatterplot(prefix, feature=di['features'][i], vmin=di['vmin'][i], vmax=di['vmax'][i],\n",
    "                                    remote_folder=remote_folder, bucket='ccurtis.data')\n",
    "\n",
    "            hm.plot_trajectories(prefix, remote_folder=remote_folder, bucket='ccurtis.data')\n",
    "            try:\n",
    "                hm.plot_histogram(prefix, remote_folder=remote_folder, bucket='ccurtis.data')\n",
    "            except ValueError:\n",
    "                print(\"Couldn't plot histogram.\")\n",
    "            hm.plot_particles_in_frame(prefix, remote_folder=remote_folder, bucket='ccurtis.data')\n",
    "            gmean1, gSEM1 = hm.plot_individual_msds(prefix, alpha=0.05, remote_folder=remote_folder, bucket='ccurtis.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_installs=('https://github.com/ccurtis7/diff_classifier.git')\n",
    "my_image = ck.DockerImage(func=download_split_track_msds, base_image='arokem/python3-fiji:0.3', github_installs=github_installs)\n",
    "\n",
    "docker_file = open(my_image.docker_path)\n",
    "docker_string = docker_file.read()\n",
    "docker_file.close()\n",
    "\n",
    "req = open(op.join(op.split(my_image.docker_path)[0], 'requirements.txt'))\n",
    "req_string = req.read()\n",
    "req.close()\n",
    "\n",
    "new_req = req_string[0:req_string.find('\\n')-3]+'5.28'+ req_string[req_string.find('\\n'):]\n",
    "req_overwrite = open(op.join(op.split(my_image.docker_path)[0], 'requirements.txt'), 'w')\n",
    "req_overwrite.write(new_req)\n",
    "req_overwrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Docker Image\n",
    "my_image.build(\"0.1\", image_name=\"test_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes to be loaded: 0\n"
     ]
    },
    {
     "ename": "CloudknotInputError",
     "evalue": "The requested bucket name already exists and you do not have permission to put or get objects in it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCloudknotInputError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f87b80af2cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                \u001b[0mbid_percentage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                \u001b[0mimage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ami-6d8a7510'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                pars_policies=('AdministratorAccess',))\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mresult_futures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_track\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mstart_knot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_knot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/source/cloudknot/cloudknot/cloudknot.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, pars, pars_policies, docker_image, base_image, func, image_script_path, image_work_dir, image_github_installs, username, repo_name, image_tags, job_definition_name, job_def_vcpus, memory, retries, compute_environment_name, instance_types, resource_type, min_vcpus, max_vcpus, desired_vcpus, image_id, ec2_key_pair, ce_tags, bid_percentage, job_queue_name, priority)\u001b[0m\n\u001b[1;32m   1337\u001b[0m             )\n\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpars_cleanup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pars'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m             futures['compute-environment'] = executor.submit(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/concurrent/futures/_base.pyc\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/concurrent/futures/thread.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/source/cloudknot/cloudknot/cloudknot.pyc\u001b[0m in \u001b[0;36mset_pars\u001b[0;34m(knot_name, input_pars, pars_policies)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                     \u001b[0mpars_cleanup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                     \u001b[0mpars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mknot_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpars_policies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                     mod_logger.info('knot {name:s} created PARS {p:s}'.format(\n",
      "\u001b[0;32m/home/ubuntu/source/cloudknot/cloudknot/cloudknot.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, batch_service_role_name, ecs_instance_role_name, ecs_task_role_name, spot_fleet_role_name, policies, vpc_id, vpc_name, use_default_vpc, security_group_id, security_group_name)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_service_role\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_service_role'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ecs_instance_role\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ecs_instance_role'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ecs_task_role\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ecs_task_role'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/concurrent/futures/_base.pyc\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/concurrent/futures/thread.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/source/cloudknot/cloudknot/cloudknot.pyc\u001b[0m in \u001b[0;36mset_role\u001b[0;34m(pars_name, role_name, service, policies, add_instance_profile)\u001b[0m\n\u001b[1;32m    321\u001b[0m                         \u001b[0mservice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                         \u001b[0mpolicies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                         \u001b[0madd_instance_profile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_instance_profile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m                     )\n\u001b[1;32m    325\u001b[0m                     mod_logger.info('PARS {name:s} created role {role:s}'\n",
      "\u001b[0;32m/home/ubuntu/source/cloudknot/cloudknot/aws/iam.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, description, service, policies, add_instance_profile)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# Add the cloudknot s3 policy if not already in input_policies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0ms3_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_s3_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             self._policies = tuple(\n\u001b[1;32m    148\u001b[0m                 \u001b[0minput_policies\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms3_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/source/cloudknot/cloudknot/aws/base_classes.pyc\u001b[0m in \u001b[0;36mget_s3_params\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Use set_s3_params to check for name availability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# and write to config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mset_s3_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/source/cloudknot/cloudknot/aws/base_classes.pyc\u001b[0m in \u001b[0;36mset_s3_params\u001b[0;34m(bucket, policy, sse)\u001b[0m\n\u001b[1;32m    255\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mclients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucketAlreadyExists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                     \u001b[0mtest_bucket_put_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;31m# Pass exception to user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/source/cloudknot/cloudknot/aws/base_classes.pyc\u001b[0m in \u001b[0;36mtest_bucket_put_get\u001b[0;34m(bucket_, sse_)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mclients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mclients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClientError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             raise CloudknotInputError('The requested bucket name already '\n\u001b[0m\u001b[1;32m    219\u001b[0m                                       \u001b[0;34m'exists and you do not have permission '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                                       'to put or get objects in it.')\n",
      "\u001b[0;31mCloudknotInputError\u001b[0m: The requested bucket name already exists and you do not have permission to put or get objects in it."
     ]
    }
   ],
   "source": [
    "to_track = []\n",
    "for slic in slices:\n",
    "    for region in regions:\n",
    "        for video in videos:\n",
    "            prefix = '100nm_S{}_XY{}_{}'.format(slic, region, video)\n",
    "\n",
    "test_length = len(to_track)\n",
    "print('Number of nodes to be loaded: {}'.format(test_length))\n",
    "\n",
    "knot = ck.Knot(name='download_and_track_{}'.format(start_knot),\n",
    "               docker_image = my_image,\n",
    "               memory = 32000,\n",
    "               resource_type = \"SPOT\",\n",
    "               bid_percentage = 100,\n",
    "               image_id = 'ami-6d8a7510',\n",
    "               pars_policies=('AdministratorAccess',))\n",
    "result_futures = knot.map(to_track)\n",
    "start_knot = start_knot + 1\n",
    "print('Next knot name: {}'.format(start_knot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot.clobber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '100nm_S1_XY2_5'\n",
    "download_split_track_msds(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/home/ubuntu/source/diff-classifier/notebooks'\n",
    "fname = 'features_P1_S1_L_0000.csv'\n",
    "file = '{}/{}'.format(folder, fname)\n",
    "features = pd.read_csv(file, index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = ['1', '2']\n",
    "regions = [1, 2, 3]\n",
    "videos = [1, 2, 3, 4, 5]\n",
    "\n",
    "for slic in slices:\n",
    "    for region in regions:\n",
    "        for video in videos:\n",
    "            prefix = '100nm_S{}_XY{}_{}'.format(slic, region, video)\n",
    "            download_split_track_msds(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_folder = \"Tissue_Studies/04_23_18_Registration_Test/tracking\"\n",
    "local_folder = os.getcwd()\n",
    "ires = 512\n",
    "frames = 651\n",
    "filename = '{}.tif'.format(prefix)\n",
    "remote_name = remote_folder+'/'+filename\n",
    "local_name = local_folder+'/'+filename\n",
    "\n",
    "msd_file = 'msd_{}.csv'.format(prefix)\n",
    "ft_file = 'features_{}.csv'.format(prefix)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "names = []\n",
    "for i in range(0, 4):\n",
    "    for j in range(0, 4):\n",
    "        names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "try:\n",
    "    obj = s3.head_object(Bucket='ccurtis.data', Key=remote_folder+'/'+ft_file)\n",
    "except:\n",
    "\n",
    "    try:\n",
    "        for name in names:\n",
    "            aws.download_s3(remote_folder+'/'+name, name, bucket_name='ccurtis.data')\n",
    "    except:\n",
    "        aws.download_s3(remote_name, local_name, bucket_name='ccurtis.data')\n",
    "        names = ij.partition_im(local_name)\n",
    "        names = []\n",
    "        for i in range(0, 4):\n",
    "            for j in range(0, 4):\n",
    "                names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "        for name in names:\n",
    "            aws.upload_s3(name, remote_folder+'/'+name, bucket_name='ccurtis.data')\n",
    "            print(\"Done with splitting.  Should output file of name {}\".format(remote_folder+'/'+name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm feat*\n",
    "!rm msd*\n",
    "!rm Traj*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '100nm_S1_XY2_5_1_1'\n",
    "local_folder = '.'\n",
    "name = \"{}.tif\".format(prefix)\n",
    "outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "local_im = op.join(local_folder, name)\n",
    "quality = 0\n",
    "y=511\n",
    "ij.track(local_im, outfile, template=None, fiji_bin=None, radius=5, threshold=0.1,\n",
    "                         do_median_filtering=True, quality=quality, x=511, y=y, ylo=1, median_intensity=300.0, snr=0.0,\n",
    "                         linking_max_distance=10.0, gap_closing_max_distance=12.0, max_frame_gap=9,\n",
    "                         track_displacement=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'P1_S1_L_0000_1_1'\n",
    "filename = 'P1_S1_L_0000_1_1.tif'\n",
    "fname = \"01_18_Experiment/P1/{}\".format(filename)\n",
    "\n",
    "aws.download_s3(fname, filename, bucket_name='ccurtis.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_folder = '.'\n",
    "name = \"{}.tif\".format(prefix)\n",
    "outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "local_im = op.join(local_folder, name)\n",
    "quality = 4.5\n",
    "y=511\n",
    "ij.track(local_im, outfile, template=None, fiji_bin=None, radius=4.5, threshold=0,\n",
    "                         do_median_filtering=True, quality=quality, x=511, y=y, ylo=1, median_intensity=300.0, snr=0.0,\n",
    "                         linking_max_distance=10.0, gap_closing_max_distance=12.0, max_frame_gap=9,\n",
    "                         track_displacement=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
